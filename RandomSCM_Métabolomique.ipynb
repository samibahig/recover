{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomSCM_MÃ©tabolomique",
      "provenance": [],
      "authorship_tag": "ABX9TyNjSPglzAXdsJMrll7ekMHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/recover/blob/master/RandomSCM_M%C3%A9tabolomique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps9eLuQTBnv8",
        "outputId": "24902f04-7e9f-4317-cf9a-47f819a044ba"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "#print('\\nMETADATA :')\n",
        "data_path = ''\n",
        "metadata_filename = '/content/metadata.csv'\n",
        "meta_df = pd.read_csv(metadata_filename)\n",
        "print('-----------')\n",
        "print(meta_df)\n",
        "print('-----------')\n",
        "#print(meta_df.columns)\n",
        "meta_df.columns = ['#', 'plate', '-', 'symptoms'] + list(meta_df)[4:]\n",
        "#print(meta_df.columns)\n",
        "#print('available metadata :', list(meta_df))\n",
        "meta_idx = meta_df['ID'].to_list()\n",
        "meta_label = meta_df['symptoms'].to_list()\n",
        "#print('------------------')\n",
        "#print(list(zip(meta_idx, meta_label)))\n",
        "#print('------------------')\n",
        "meta_id_label_dict = {str(k): 1 if v=='S' else 0 for k, v in zip(meta_idx, meta_label)}\n",
        "\n",
        "\n",
        "print('\\nMETABOLOMICS DATA :')\n",
        "metabolomics_data_filename = '/content/metabolomics.csv'\n",
        "#print('data_path', '/content/metabolomics.csv')\n",
        "feat_df = pd.read_csv(metabolomics_data_filename, index_col=0, skiprows=[0], dtype=str, usecols=[0])\n",
        "features = list(feat_df.index.values)\n",
        "print('# of features : ', len(features))\n",
        "print('first feature :', features[0])\n",
        "print('last feature :', features[-1])\n",
        "col_list = [\"Group\"]\n",
        "column1 = pd.read_csv(metabolomics_data_filename, usecols=col_list)\n",
        "print('----------')\n",
        "column1 = column1['Group'].to_list()\n",
        "print(column1[1:])\n",
        "\n",
        "print('----------')\n",
        "idx_df = pd.read_csv(metabolomics_data_filename, header=1, nrows=1, encoding='unicode_escape')\n",
        "\n",
        "idx = list(idx_df)[1:]\n",
        "idx = [l[17:22] for l in idx]\n",
        "labels_df = pd.read_csv(metabolomics_data_filename, nrows=1)\n",
        "labels = list(labels_df)[1:]\n",
        "print('# of labels : ', len(labels))\n",
        "\n",
        "cols_df = pd.read_csv(metabolomics_data_filename, header=1, nrows=1, encoding='unicode_escape')\n",
        "cols_list = list(cols_df)\n",
        "\n",
        "df3 = pd.read_csv(metabolomics_data_filename, header=1, na_values=['#DIV/0!'], usecols=cols_list[1:], encoding='unicode_escape')\n",
        "df3 = df3.T\n",
        "df3['idx'] = idx\n",
        "df3.set_index('idx', inplace=True)\n",
        "df3.columns = features\n",
        "df3 = df3.dropna(axis=1)\n",
        "\n",
        "#clean data of samples that are not in metadata :\n",
        "idx = df3.index.values\n",
        "y = []\n",
        "for k in range(len(idx)):\n",
        "    id = idx[k]\n",
        "    if id in meta_id_label_dict:\n",
        "        y.append(meta_id_label_dict[id])\n",
        "    else:\n",
        "        # we will not put this sample in the dataset\n",
        "        #print('sample to remove because of unknown label:', k, id)\n",
        "        y.append('to_remove')\n",
        "df3['label'] = y\n",
        "df3 = df3[df3.label != 'to_remove']\n",
        "\n",
        "#create X and y matrices for ML :\n",
        "y = list(df3['label'])\n",
        "print(y)\n",
        "del df3['label']\n",
        "X = df3.to_numpy()\n",
        "\n",
        "print('metabolomics data :')\n",
        "print('# of samples : ', df3.shape[0])\n",
        "print('# of features : ', df3.shape[1])\n",
        "print('labels:', list(dict.fromkeys(y)))\n",
        "\n",
        "## to concatenate the 2 proteomics dataframes and the metabolomics if you want :\n",
        "##df = pd.concat([df_1_2, df3], axis=1)\n",
        "##df = df.dropna(axis=0)\n",
        "##print('multi-omics df :')\n",
        "##print('# of samples : ', df.shape[0])\n",
        "##print('# of features : ', df.shape[1])\n",
        "\n",
        "from sklearn.base import ClassifierMixin\n",
        "from sklearn.ensemble import BaseEnsemble\n",
        "from pyscm import SetCoveringMachineClassifier\n",
        "from pyscm.rules import DecisionStump\n",
        "\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\n",
        "from sklearn.utils.random import sample_without_replacement\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numbers\n",
        "import itertools\n",
        "import numpy as np\n",
        "from warnings import warn\n",
        "from joblib import effective_n_jobs, Parallel, delayed\n",
        "\n",
        "MAX_INT = np.iinfo(np.int32).max\n",
        "def _parallel_build_estimators(idx, ensemble, p_of_estims, seeds, X, y, tiebreaker):\n",
        "    \"\"\"\n",
        "    Fit SCM estimators on subsamples of the training data\n",
        "    \"\"\"\n",
        "    estimators = []\n",
        "    estim_features = []\n",
        "    for k in idx:\n",
        "        p_param = p_of_estims[k] # p param for the classifier to fit\n",
        "        random_state = seeds[k]\n",
        "        estim = SetCoveringMachineClassifier(p=p_param, max_rules=ensemble.max_rules, model_type=ensemble.model_type, random_state=random_state)\n",
        "        feature_indices = sample_without_replacement(ensemble._pop_features, ensemble._max_features, random_state=random_state)\n",
        "        samples_indices = sample_without_replacement(ensemble._pop_samples, ensemble._max_samples, random_state=random_state)\n",
        "        Xk = (X[samples_indices])[:, feature_indices]\n",
        "        yk = y[samples_indices]\n",
        "        if len(list(set(yk))) < 2:\n",
        "            raise ValueError(\"One of the subsamples contains elements from only 1 class, try increase max_samples value\")\n",
        "        if tiebreaker is None:\n",
        "            estim.fit(Xk, yk)\n",
        "        else:\n",
        "            estim.fit(Xk, yk, tiebreaker)\n",
        "        estim_features.append(feature_indices)\n",
        "        estimators.append(estim)\n",
        "    return estimators, estim_features\n",
        "\n",
        "\n",
        "def _parallel_predict_proba(ensemble, X, idx, results):\n",
        "    \"\"\"\n",
        "    Compute predictions of SCM estimators\n",
        "    \"\"\"\n",
        "    for k in idx:\n",
        "        res = ensemble.estimators[k].predict(X[:, ensemble.estim_features[k]])\n",
        "        if ensemble.min_cq_combination:\n",
        "            res[res==0] = -1\n",
        "            results = results + ensemble.min_cq_weights[k]*res\n",
        "        else:\n",
        "            results = results + res\n",
        "    return results\n",
        "\n",
        "\n",
        "def _partition_estimators(n_estimators, n_jobs):\n",
        "    \"\"\"Private function used to partition estimators between jobs.\"\"\"\n",
        "    # Compute the number of jobs\n",
        "    n_jobs = min(effective_n_jobs(n_jobs), n_estimators)\n",
        "\n",
        "    # Partition estimators between jobs\n",
        "    n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs,\n",
        "                                   dtype=np.int)\n",
        "    n_estimators_per_job[:n_estimators % n_jobs] += 1\n",
        "    starts = np.cumsum(n_estimators_per_job)\n",
        "\n",
        "    return n_jobs, n_estimators_per_job.tolist(), [0] + starts.tolist()\n",
        "\n",
        "\n",
        "class RandomScmClassifier(BaseEnsemble, ClassifierMixin):\n",
        "    \"\"\"A Bagging classifier for SetCoveringMachineClassifier()\n",
        "    The base estimators are built on subsets of both samples\n",
        "    and features.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int, default=10\n",
        "        The number of base estimators in the ensemble.\n",
        "    max_samples : int or float, default=1.0\n",
        "        The number of samples to draw from X to train each base estimator with\n",
        "        replacement.\n",
        "        - If int, then draw `max_samples` samples.\n",
        "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
        "    max_features : int or float, default=1.0\n",
        "        The number of features to draw from X to train each base estimator (\n",
        "        without replacement.\n",
        "        - If int, then draw `max_features` features.\n",
        "        - If float, then draw `max_features * X.shape[1]` features.\n",
        "    max_rules : int\n",
        "        maximal number of rules for the scm estimators\n",
        "    p_options : list of float with len =< n_estimators, default=[1.0]\n",
        "        The estimators will be fitted with values of p found in p_options\n",
        "        let k be k = n_estimators/len(p_options),\n",
        "        the k first estimators will have p=p_options[0],\n",
        "        the next k estimators will have p=p_options[1] and so on...\n",
        "    model_type : string, default='conjunction'\n",
        "        type of estimators to build\n",
        "        accepted values : 'conjunction', 'disjunction'\n",
        "    n_jobs : int, default=None\n",
        "        The number of jobs to run in parallel for both fit and\n",
        "        predict. 'None' means 1. '-1' means using all\n",
        "        processors.\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls the random resampling of the original dataset\n",
        "        (sample wise and feature wise).\n",
        "        If the base estimator accepts a `random_state` attribute, a different\n",
        "        seed is generated for each instance in the ensemble.\n",
        "        Pass an int for reproducible output across multiple function calls.\n",
        "        See :term:`Glossary <random_state>`.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    n_features_ : int\n",
        "        The number of features when :meth:`fit` is performed.\n",
        "    estimators_ : list of estimators\n",
        "        The collection of fitted base estimators.\n",
        "    estim_features : list of arrays\n",
        "        The subset of drawn features for each base estimator.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> random_scm = RandomScmClassifier(p_options=[2, 4], max_samples=0.5, max_features = 0.7)\n",
        "    >>> random_scm.fit(X_train, y_train)\n",
        "    >>> hyperparams = random_scm.get_hyperparams()\n",
        "    >>> importances = random_scm.features_importance()\n",
        "    >>> disagree = random_scm.classifiers_disagreement(X)\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
        "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
        "    .. [2] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
        "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_estimators=100,\n",
        "                 max_samples=0.5,\n",
        "                 max_features=0.5,\n",
        "                 max_rules=10,\n",
        "                 p_options=[1.0],\n",
        "                 model_type=\"conjunction\",\n",
        "                 n_jobs=None,\n",
        "                 min_cq_combination=False,\n",
        "                 min_cq_mu = 10e-3,\n",
        "                 random_state=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.max_rules = max_rules\n",
        "        self.p_options = p_options\n",
        "        self.model_type = model_type\n",
        "        self.n_jobs = n_jobs\n",
        "        self.min_cq_combination = min_cq_combination\n",
        "        self.min_cq_mu = min_cq_mu\n",
        "        self.random_state = random_state\n",
        "        self.labels_to_binary = {}\n",
        "        self.binary_to_labels = {}\n",
        "\n",
        "    def p_for_estimators(self):\n",
        "        \"\"\"Return the value of p for each estimator to fit.\"\"\"\n",
        "        options_len = len(self.p_options) # number of options\n",
        "        estims_with_same_p = self.n_estimators // options_len # nb of estimators to fit with the same p\n",
        "        p_of_estims = []\n",
        "        if options_len > 1:\n",
        "            for k in range(options_len - 1):\n",
        "                opt = self.p_options[k] # an option\n",
        "                p_of_estims = p_of_estims + ([opt] * estims_with_same_p) # estims_with_same_p estimators with p=opt\n",
        "        p_of_estims = p_of_estims + ([self.p_options[-1]] * (self.n_estimators - len(p_of_estims)))\n",
        "        return p_of_estims\n",
        "\n",
        "    def get_estimators(self):\n",
        "        \"\"\"Return the list of estimators of the classifier\"\"\"\n",
        "        if hasattr(self, 'estimators'):\n",
        "            return self.estimators\n",
        "        else:\n",
        "            return \"not defined (model not fitted)\"\n",
        "\n",
        "    def get_hyperparams(self):\n",
        "        \"\"\"Return the model hyperparameters\"\"\"\n",
        "        hyperparams = {\n",
        "            'n_estimators' : self.n_estimators, \n",
        "            'max_samples' : self.max_samples, \n",
        "            'max_features' : self.max_features, \n",
        "            'max_rules' : self.max_rules, \n",
        "            'p_options' : self.p_options, \n",
        "            'model_type' : self.model_type, \n",
        "            'random_state' : self.random_state\n",
        "        }\n",
        "        return hyperparams\n",
        "\n",
        "    def labels_conversion(self, labels_list):\n",
        "        \"\"\"\n",
        "        Return the equivalence between labels and binaries\n",
        "        \"\"\"\n",
        "        l = list(set(labels_list))\n",
        "        labels_dict = {c:idx for idx, c in enumerate(l)}\n",
        "        if len(l) < 2:\n",
        "            raise ValueError(\"Only 1 classe given to the model, needs 2\")\n",
        "        elif len(l) > 2:\n",
        "             raise ValueError(\"{} classes were given, multiclass prediction is not implemented\".format(len(l)))\n",
        "        return np.array(l), labels_dict\n",
        "\n",
        "\n",
        "    def fit(self, X, y, get_feature_importances=True, tiebreaker=None):\n",
        "        \"\"\"\n",
        "        Fit the model with the given data\n",
        "        \"\"\"\n",
        "        # Check if 2 classes are inputed and convert labels to binary labels\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.classes_, self.labels_to_binary = self.labels_conversion(y)\n",
        "        self.binary_to_labels = {bin_label:str_label for str_label, bin_label in self.labels_to_binary.items()}\n",
        "        y = np.array([self.labels_to_binary[l] for l in y])\n",
        "\n",
        "        # Save the original number of features\n",
        "        self.n_features = X.shape[1]\n",
        "\n",
        "        self.estimators = []\n",
        "        self.estim_features = []\n",
        "        max_rules = self.max_rules\n",
        "        p_of_estims_ = self.p_for_estimators()\n",
        "        model_type = self.model_type\n",
        "\n",
        "        #seeds for reproductibility\n",
        "        random_state = self.random_state\n",
        "        random_state = check_random_state(random_state)\n",
        "        seeds = random_state.randint(MAX_INT, size=self.n_estimators)\n",
        "        self._seeds = seeds\n",
        "\n",
        "        pop_samples, pop_features = X.shape\n",
        "        max_samples, max_features = self.max_samples, self.max_features\n",
        "\n",
        "        # validate max_samples\n",
        "        if not isinstance(max_samples, numbers.Integral):\n",
        "            max_samples = int(max_samples * pop_samples)\n",
        "        if not (0 < max_samples <= pop_samples):\n",
        "            raise ValueError(\"max_samples must be in (0, n_samples)\")\n",
        "        # store validated integer row sampling values\n",
        "        self._max_samples = max_samples\n",
        "        self._pop_samples = pop_samples\n",
        "\n",
        "        # validate max_features\n",
        "        if isinstance(self.max_features, numbers.Integral):\n",
        "            max_features = self.max_features\n",
        "        elif isinstance(self.max_features, np.float):\n",
        "            max_features = self.max_features * pop_features\n",
        "        else:\n",
        "            raise ValueError(\"max_features must be int or float\")\n",
        "        if not (0 < max_features <= pop_features):\n",
        "            raise ValueError(\"max_features must be in (0, n_features)\")\n",
        "        max_features = max(1, int(max_features))\n",
        "        # store validated integer feature sampling values\n",
        "        self._max_features = max_features\n",
        "        self._pop_features = pop_features\n",
        "\n",
        "        # parallel loop\n",
        "        n_jobs, n_estimators_list, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n",
        "        # building estimators\n",
        "        all_results = Parallel(n_jobs=n_jobs)(delayed(_parallel_build_estimators)(\n",
        "                range(starts[i],starts[i+1]), self, p_of_estims_, seeds, X, y, tiebreaker)\n",
        "            for i in range(n_jobs))\n",
        "\n",
        "        self.estimators += list(itertools.chain.from_iterable(\n",
        "            t[0] for t in all_results))\n",
        "        self.estim_features += list(itertools.chain.from_iterable(\n",
        "            t[1] for t in all_results))\n",
        "\n",
        "        if self.min_cq_combination:\n",
        "            predictions = np.zeros((X.shape[0], self.n_estimators))\n",
        "            for k in range(self.n_estimators):\n",
        "                predictions[:, k] = self.estimators[k].predict(X[:, self.estim_features[k]])\n",
        "\n",
        "            from .min_cq import MinCqLearner\n",
        "            mincq = MinCqLearner(self.min_cq_mu, \"stumps\", n_stumps_per_attribute=1,\n",
        "                                 self_complemented=False)\n",
        "            mincq.fit(predictions, y)\n",
        "            self.min_cq_weights = mincq.majority_vote.weights\n",
        "            self.min_cq_weights /= np.sum(np.abs(self.min_cq_weights))\n",
        "\n",
        "        if get_feature_importances:\n",
        "            importances = self.features_importance()\n",
        "            self.feature_importances_ = np.array([importances['avg'][k]\n",
        "                                                  if k in importances['avg'] else 0\n",
        "                                                  for k in range(self.n_features)])\n",
        "            self.feature_importances_ /= np.sum(self.feature_importances_)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Compute model predictions for data in X\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        predictions : array\n",
        "            predictions[i] is the predicted class for he sample i in X\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X = check_array(X)\n",
        "        predicted_proba = self.predict_proba(X)\n",
        "        predictions = np.array(np.argmax(predicted_proba, axis=1), dtype=int)\n",
        "        predictions = np.array([self.binary_to_labels[l] for l in predictions])\n",
        "        return predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities according to the model estimators\n",
        "\n",
        "        Parameters :\n",
        "        ----------\n",
        "        X : array\n",
        "            a dataset to predict\n",
        "\n",
        "        Returns : \n",
        "        ----------\n",
        "        proba : array\n",
        "            proba[c] contains the estimated probability for each sample of X to belong to class c\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X = check_array(X)\n",
        "        # parallel loop\n",
        "        n_jobs, n_estimators_list, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n",
        "        results = np.zeros(X.shape[0])\n",
        "        results = Parallel(n_jobs=n_jobs)(delayed(_parallel_predict_proba)(\n",
        "                self, X, range(starts[i], starts[i+1]), results)\n",
        "                for i in range(n_jobs))\n",
        "        if not self.min_cq_combination:\n",
        "            votes = sum(results) / self.n_estimators\n",
        "            proba = np.array([np.array([1 - vote, vote]) for vote in votes])\n",
        "        else:\n",
        "            votes = sum(results)\n",
        "            proba = np.array([np.array([(1 - vote)/2, (1 + vote)/2]) for vote in votes])\n",
        "        return proba\n",
        "\n",
        "    def features_importance(self):\n",
        "        \"\"\"\n",
        "        Compute features importances in estimators rules\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        importance : dict\n",
        "            importances['avg'] : dict (feature id as key, mean importance as value)\n",
        "                The mean importance of each feature over the estimators.\n",
        "            importances['max'] : dict (feature id as key, max importance as value)\n",
        "                The maximal importance of each feature over the estimators.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        importances = {'avg' : {}, 'max' : {}} # average and maximal feature/rule importances\n",
        "        feature_id_occurences = {} # number of occurences of a feature in subsamples\n",
        "        for (estim, features_idx) in zip(self.estimators, self.estim_features):\n",
        "            # increment the total occurences of the feature :\n",
        "            for id_feat in features_idx:\n",
        "                if id_feat in feature_id_occurences:\n",
        "                    feature_id_occurences[id_feat] += 1\n",
        "                else:\n",
        "                    feature_id_occurences[id_feat] = 1\n",
        "            # sum the rules importances :\n",
        "            #rules_importances = estim.get_rules_importances() # activate it when pyscm will implement importance\n",
        "            rules_importances = np.ones(len(estim.model_.rules)) #delete it when pyscm will implement importance\n",
        "            for rule, importance in zip(estim.model_.rules, rules_importances):\n",
        "                global_feat_id = features_idx[rule.feature_idx]\n",
        "                if global_feat_id in importances['avg']:\n",
        "                    importances['avg'][global_feat_id] += importance\n",
        "                    if importance > importances['max'][global_feat_id]:\n",
        "                        importances['max'][global_feat_id] = importance\n",
        "                else:\n",
        "                    importances['avg'][global_feat_id] = importance\n",
        "                    importances['max'][global_feat_id] = importance\n",
        "        importances['avg'] = {k: round(v / feature_id_occurences[k], 3) for k, v in importances['avg'].items()}\n",
        "        return importances\n",
        "\n",
        "    def all_data_tiebreaker(self, model_type, feature_idx, thresholds, rule_type, X, y):\n",
        "        \"\"\"\n",
        "        Choose a rule between rule with equal utility\n",
        "        Select the one that have the best accuracy if applied alone on all the data\n",
        "\n",
        "        Parameters :\n",
        "        ----------\n",
        "        model_type : strint ('conjunction' or 'dijunction')\n",
        "            type of the model\n",
        "        feature_idx, thresholds, rule_type : arrays\n",
        "            description of the rules\n",
        "        X, y : arrays\n",
        "            a dataset used to compare rules\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        ID of the rule to select\n",
        "        \"\"\"\n",
        "        keep_id = 0\n",
        "        keep_id_score = -1\n",
        "        for k in range(len(feature_idx)):\n",
        "            feat_id, threshold, r_type = feature_idx[k], thresholds[k], rule_type[k]\n",
        "            stump = DecisionStump(feature_idx=feat_id, threshold=threshold, kind=r_type)\n",
        "            rule_classif = stump.classify(X).astype('int')\n",
        "            rule_global_score = (rule_classif == y).sum()\n",
        "            if rule_global_score > keep_id_score:\n",
        "                keep_id = k\n",
        "                keep_id_score = rule_global_score\n",
        "        return keep_id\n",
        "\n",
        "    def classifiers_disagreement(self, X):\n",
        "        \"\"\"\n",
        "        Compute disagreement between estimators\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        disagreement : dict \n",
        "            disagreement['global'] : int\n",
        "                global disagreement, computed with the following formula :\n",
        "                average(disagreement(c1, c2))\n",
        "                for (c1, c2) all the pairs of distinct estimators in the model\n",
        "                with \n",
        "                disagreement(c1, c2) = (# of examples where c1 and c2 differs)/(# of examples)\n",
        "            disagreement['estims'] : list\n",
        "                list of disagreements of each estimator with the global model\n",
        "            disagreement['heatmap'] : matrix\n",
        "                heatmap of mutual disagreement for each pair of classifiers\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        disagreement = {'global' : 0, 'estims' : []}\n",
        "        total_comp = 0 # number of comparison of estimators\n",
        "        global_pred = self.predict(X)\n",
        "        heatmap = np.zeros((self.n_estimators, self.n_estimators))\n",
        "        for kA in range(len(self.estimators)):\n",
        "            estimA = self.estimators[kA]\n",
        "            predA = estimA.predict(X[:, self.estim_features[kA]])\n",
        "            disagree = np.not_equal(global_pred, predA).sum()/len(X) # disagreement of estimator A and global model\n",
        "            disagreement['estims'].append(disagree)\n",
        "            for kB in range(len(self.estimators)):\n",
        "                if kB == kA:\n",
        "                    continue # do not compare the estimator with itself\n",
        "                estimB = self.estimators[kB]\n",
        "                predB = estimB.predict(X[:, self.estim_features[kB]])\n",
        "                disagree = np.not_equal(predB, predA).sum()/len(X) # disagreement of estimators A and B\n",
        "                heatmap[kA][kB] = disagree\n",
        "                disagreement['global'] += disagree\n",
        "                total_comp += 1\n",
        "        disagreement['global'] = disagreement['global']/total_comp\n",
        "        disagreement['heatmap'] = heatmap\n",
        "        return disagreement\n",
        "    \n",
        "    def get_estimators_indices(self):\n",
        "        \"\"\"\n",
        "        Get drawn indices along both sample and feature axes\n",
        "        \"\"\"\n",
        "        for seed in self._seeds:\n",
        "            # operations accessing random_state must be performed identically to those in 'fit'\n",
        "            feature_indices = sample_without_replacement(self._pop_features, self._max_features, random_state=seed)\n",
        "            samples_indices = sample_without_replacement(self._pop_samples, self._max_samples, random_state=seed)\n",
        "            yield samples_indices\n",
        "\n",
        "    def score(self, X, y):\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X, y = check_X_y(X, y)\n",
        "        return accuracy_score(y, self.predict(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "    Unnamed: 0 Unnamed: 1  ... addition                                specify.1\n",
            "0            1    Plate 1  ...      NaN                                      NaN\n",
            "1            2    Plate 1  ...      5.0                                   Stress\n",
            "2            3    Plate 1  ...      6.0  back pain is worse, shortness of breath\n",
            "3            4    Plate 1  ...      2.0  Impression of a foreingh body in throat\n",
            "4            5    Plate 1  ...      2.0                                      NaN\n",
            "..         ...        ...  ...      ...                                      ...\n",
            "95         100    Plate 2  ...      NaN                                      NaN\n",
            "96         101    Plate 2  ...      NaN                                      NaN\n",
            "97         102    Plate 2  ...      NaN                                      NaN\n",
            "98         103    Plate 2  ...      NaN                                      NaN\n",
            "99         104    Plate 2  ...      NaN                                      NaN\n",
            "\n",
            "[100 rows x 46 columns]\n",
            "-----------\n",
            "\n",
            "METABOLOMICS DATA :\n",
            "# of features :  483\n",
            "first feature : 3.15_160.1695m/z\n",
            "last feature : 8.54_397.4147m/z\n",
            "----------\n",
            "['3.15_160.1695m/z', '8.56_366.6297m/z', '6.52_828.4067n', '7.23_281.0516m/z', '8.34_429.0893m/z', '4.55_252.0298m/z', '4.56_234.0194m/z', '4.55_220.0590m/z', '5.19_386.0794m/z', '8.56_367.6297m/z', '4.55_337.0498n', '7.68_358.3671m/z', '5.07_371.2588m/z', '5.87_393.0866m/z', '8.34_485.1126m/z', '8.34_445.1197m/z', '4.04_133.0143m/z', '7.68_355.0720m/z', '8.87_519.1391m/z', '4.54_352.0727m/z', '8.34_248.0596m/z', '8.34_355.0706m/z', '5.19_370.2564m/z', '4.78_354.0894m/z', '7.68_371.1045m/z', '9.58_445.1201m/z', '7.68_281.0517m/z', '8.34_281.0516m/z', '8.34_371.1021m/z', '3.24_268.0614m/z', '5.28_429.2386m/z', '7.68_248.0590m/z', '4.04_151.0293m/z', '4.08_284.0562m/z', '8.87_206.0254n', '8.38_207.0326m/z', '6.52_414.2042n', '8.34_223.0674m/z', '4.60_368.0878m/z', '8.87_559.1315m/z', '7.68_190.0325n', '7.68_411.1100m/z', '6.52_119.0859m/z', '5.87_449.1529m/z', '8.87_355.0708m/z', '9.58_610.1621n', '9.58_355.0707m/z', '6.89_208.0098n', '9.58_207.0320m/z', '3.59_546.1151m/z', '6.90_248.0587m/z', '8.87_371.1010m/z', '8.69_425.2868m/z', '6.89_281.0510m/z', '9.57_371.1004m/z', '0.67_134.1178m/z', '0.65_118.1229m/z', '6.89_297.0853m/z', '6.89_207.0285n', '3.90_72.0406n', '8.56_636.3211n', '4.87_400.0771m/z', '2.36_152.1073m/z', '0.70_173.0793m/z', '7.60_281.1721m/z', '9.06_499.3644m/z', '5.28_192.0757m/z', '10.77_158.0025m/z', '10.58_629.3145m/z', '2.97_114.0916m/z', '6.90_266.9992m/z', '10.61_134.1180m/z', '9.58_281.0515m/z', '6.43_367.1787m/z', '10.77_101.0060n', '4.59_263.0298m/z', '10.75_96.9619n', '4.37_384.0827m/z', '10.66_132.9213n', '4.60_216.0164m/z', '10.78_451.7890m/z', '10.76_70.9582m/z', '10.61_86.0968m/z', '10.66_130.9230n', '10.58_857.6651m/z', '10.79_407.7800n', '10.77_452.7776n', '10.75_77.9397n', '10.64_135.9270m/z', '1.13_151.0021n', '9.34_932.7696n', '0.68_194.1177m/z', '10.77_54.9278n', '10.75_217.8598m/z', '7.71_278.1521n', '0.68_140.1063m/z', '8.93_859.6424m/z', '10.64_84.9600m/z', '8.93_847.5541n', '2.35_231.0199n', '8.93_853.6161n', '7.15_439.1371m/z', '0.56_129.0653m/z', '0.48_734.8156m/z', '10.79_116.9280n', '5.66_385.1839m/z', '6.50_457.2038m/z', '10.58_601.1924m/z', '10.82_234.8638m/z', '8.55_853.5809m/z', '2.51_232.0272m/z', '10.37_489.2363m/z', '5.37_196.0059m/z', '10.79_363.7896m/z', '9.14_779.5228m/z', '0.48_67.9882n', '8.87_926.6732n', '2.01_224.1278m/z', '9.20_845.6331m/z', '4.37_200.0379m/z', '8.50_769.5411m/z', '8.93_244.1448n', '8.80_956.7144n', '8.66_801.6043m/z', '10.77_206.9166m/z', '8.82_797.5648m/z', '8.93_878.6353m/z', '7.71_121.0287m/z', '10.76_94.4241n', '7.71_579.3022m/z', '2.87_283.1753m/z', '8.88_799.5979n', '8.47_283.1662m/z', '10.63_260.8617n', '8.48_941.6700m/z', '10.75_139.9644m/z', '8.66_802.5508n', '8.56_946.6860m/z', '0.70_243.0835m/z', '2.59_192.1382m/z', '10.78_93.9488n', '10.63_203.8363n', '7.71_446.1952m/z', '10.76_155.9744m/z', '7.78_453.1538m/z', '10.57_855.6807m/z', '9.57_656.2281m/z', '10.64_176.9525m/z', '10.79_71.9304n', '4.27_329.1572m/z', '8.87_937.6601m/z', '10.63_187.8434n', '10.72_102.9963n', '10.83_235.8267n', '10.56_670.9572m/z', '8.90_913.6887m/z', '9.97_641.9368m/z', '8.73_677.4803m/z', '8.49_135.0806m/z', '10.79_406.7918m/z', '2.42_100.0761m/z', '10.63_122.9640m/z', '9.67_862.7239n', '10.63_214.8348n', '3.88_326.1752m/z', '7.71_404.6627n', '2.72_110.0605m/z', '8.73_923.6350m/z', '0.48_1067.7386n', '0.51_164.1281m/z', '7.71_279.0902m/z', '8.47_878.6078n', '10.57_385.1833m/z', '10.63_126.9581m/z', '8.38_224.0637m/z', '8.70_828.6371m/z', '10.56_403.2034m/z', '0.69_218.1381m/z', '9.34_540.4034n', '7.71_149.0240m/z', '0.69_136.1122m/z', '7.55_219.1718m/z', '10.76_56.9429m/z', '1.13_268.9599m/z', '7.71_92.0266n', '10.79_84.9379m/z', '6.90_337.0758m/z', '0.66_958.2255m/z', '0.48_617.8269n', '7.71_172.0529n', '10.64_163.9897m/z', '10.81_173.8698m/z', '10.63_272.8823m/z', '8.12_989.5782m/z', '3.57_119.0614m/z', '7.71_513.1785m/z', '5.77_158.1542m/z', '8.87_943.7013m/z', '4.20_471.7376m/z', '10.76_208.9366m/z', '8.48_958.6542n', '1.13_141.0408m/z', '4.98_167.9936m/z', '8.66_783.5768m/z', '6.07_761.3044m/z', '10.69_174.9256n', '8.50_832.6035m/z', '4.19_329.1552m/z', '4.37_213.1593m/z', '0.48_802.7473n', '7.71_420.0870m/z', '0.56_83.0607m/z', '10.62_352.9661m/z', '10.59_62.9298m/z', '10.64_180.9708m/z', '10.76_144.9231n', '2.33_192.1382m/z', '0.48_397.8955n', '0.47_914.8086m/z', '10.08_338.6050m/z', '5.31_413.1841m/z', '9.04_526.3817n', '10.77_200.8703n', '2.25_100.0762m/z', '0.48_203.9625n', '10.77_262.8577m/z', '8.75_843.5869m/z', '10.56_771.6158m/z', '0.47_846.8217m/z', '0.48_625.8366n', '0.67_169.1730m/z', '5.91_290.2684m/z', '0.48_1135.7273n', '0.48_886.7633m/z', '8.42_418.3071n', '8.55_813.6217m/z', '10.64_223.9882m/z', '10.77_128.9514m/z', '0.60_130.1067m/z', '0.48_465.8831n', '9.00_652.4029n', '8.69_925.6636m/z', '6.73_374.3255m/z', '0.55_136.0483m/z', '4.92_283.1607m/z', '0.48_750.7926m/z', '8.93_1006.6831n', '0.47_982.7960m/z', '0.66_1126.2535m/z', '0.48_684.8016m/z', '5.71_311.2690m/z', '10.79_257.8791n', '8.81_418.3864n', '0.48_855.7453n', '3.57_94.0655m/z', '8.93_326.2060n', '0.48_795.7747n', '0.48_533.8705n', '0.47_158.9644m/z', '0.53_107.9670m/z', '0.48_693.8240n', '8.59_282.4561m/z', '0.47_1098.0765n', '0.47_347.8656n', '10.61_116.1072m/z', '10.82_334.7902m/z', '9.88_82.0534n', '0.48_624.8467m/z', '7.85_372.2253m/z', '10.63_147.9533m/z', '0.67_351.0138n', '10.80_318.7922m/z', '6.27_110.0828n', '10.63_281.8610m/z', '0.56_192.9792m/z', '2.14_192.0655m/z', '10.79_116.9273m/z', '5.36_418.3087m/z', '8.93_370.2567n', '8.93_365.1238m/z', '10.70_44.9656n', '7.03_388.3394m/z', '0.47_271.9495n', '8.34_373.1288m/z', '0.47_941.2694n', '2.34_124.1121m/z', '0.48_659.8004n', '0.48_737.8318n', '6.86_938.6606m/z', '10.65_270.8873m/z', '10.63_37.4615n', '0.55_114.0666m/z', '10.81_525.7150m/z', '8.99_496.2800m/z', '10.55_643.3153m/z', '5.68_193.1559m/z', '0.48_989.7416m/z', '7.71_646.2481n', '0.48_174.9382m/z', '10.58_495.1263m/z', '9.21_840.6457m/z', '9.48_649.4416m/z', '0.47_339.9374n', '0.67_390.9828m/z', '10.63_201.8489n', '10.76_398.7591m/z', '8.14_604.4163m/z', '10.63_373.7666n', '1.13_267.9525m/z', '10.81_88.9328n', '8.72_721.4967m/z', '10.63_295.8714n', '8.59_212.2008m/z', '7.71_139.0394m/z', '0.48_352.8975m/z', '7.16_437.1421m/z', '0.48_857.7832m/z', '0.47_668.8682m/z', '8.57_621.4201n', '0.47_475.9121n', '8.76_882.6281n', '10.08_338.7873m/z', '7.71_307.6203m/z', '2.20_118.0654m/z', '0.48_907.7356n', '0.47_975.3129m/z', '10.63_293.8967n', '5.87_489.2682m/z', '9.37_541.2873m/z', '4.61_229.3030m/z', '8.37_635.4242m/z', '3.57_212.1185m/z', '6.16_495.1274m/z', '0.48_557.8499n', '8.93_568.4319n', '0.47_407.9249n', '0.48_713.8277n', '7.96_541.8922m/z', '7.53_221.0869m/z', '10.69_211.8948m/z', '8.60_133.0977m/z', '0.48_761.8112n', '0.48_989.7070n', '0.67_336.0476n', '8.29_548.8999m/z', '0.48_895.7877n', '0.47_1030.0896n', '0.68_86.0968m/z', '5.71_285.2563m/z', '0.47_677.3575m/z', '8.50_281.0512m/z', '0.48_399.8924n', '6.38_778.5500m/z', '0.47_543.8997n', '6.87_360.1787n', '4.61_229.1437m/z', '8.93_1019.7181n', '4.01_329.1562m/z', '8.93_390.2493m/z', '0.48_221.9346n', '8.59_170.1542m/z', '8.47_627.4131m/z', '4.61_92.0628n', '10.63_295.8837m/z', '0.47_906.8247m/z', '7.71_276.0526m/z', '8.15_374.2481m/z', '5.07_404.2947m/z', '0.48_823.8374n', '0.47_1152.7638m/z', '8.50_268.9829m/z', '0.47_1016.7901m/z', '7.71_444.6962m/z', '0.48_467.8804n', '10.69_420.7883m/z', '8.87_628.4527n', '0.47_1058.7691m/z', '6.98_306.1874m/z', '0.47_1166.0630n', '10.61_224.1280m/z', '0.48_337.8350n', '0.47_778.8332m/z', '8.92_473.3455m/z', '0.67_518.1367m/z', '10.64_108.9618m/z', '8.59_85.1010m/z', '10.63_120.9745n', '0.67_151.0024n', '0.58_527.1587m/z', '8.93_656.5108m/z', '10.00_581.3058m/z', '6.13_676.4792m/z', '8.93_488.2715n', '0.68_168.1018m/z', '0.47_679.8739n', '0.47_611.8866n', '5.53_385.1832m/z', '7.71_925.4059m/z', '8.93_696.5371m/z', '10.63_303.8838n', '0.47_745.3445m/z', '0.53_223.8835m/z', '8.93_516.3244n', '8.93_682.4592m/z', '0.47_907.3254m/z', '0.67_1190.2489m/z', '0.67_686.1651m/z', '10.64_178.9901m/z', '8.86_405.3181m/z', '8.50_206.0253n', '0.47_872.8297m/z', '0.48_354.8947m/z', '7.71_307.1205m/z', '7.71_221.9856m/z', '0.67_1022.2205m/z', '0.53_288.0288m/z', '10.79_189.8651m/z', '5.85_510.2929n', '7.71_177.9562n', '0.05_115.9692n', '10.63_362.7651n', '5.78_518.0267m/z', '8.93_181.1227m/z', '8.68_252.1751n', '8.59_247.2409m/z', '8.35_536.4422n', '7.77_410.1314m/z', '0.67_141.0410m/z', '7.39_547.1605m/z', '4.61_96.1180n', '9.24_654.3888m/z', '8.27_388.2626m/z', '8.71_380.3253n', '8.50_290.0154m/z', '0.48_717.3384n', '10.02_517.4145m/z', '0.47_770.8503m/z', '2.60_124.0885n', '6.43_286.1435m/z', '8.00_456.2808n', '0.47_1008.8041m/z', '0.48_1032.7683m/z', '0.67_854.1927m/z', '8.79_567.3084m/z', '6.00_373.2722m/z', '7.32_390.2351m/z', '0.47_940.8167m/z', '6.20_761.3030m/z', '0.47_1084.7769m/z', '10.76_169.3997n', '0.47_838.8379m/z', '8.75_633.4582m/z', '10.63_117.9599m/z', '8.09_219.1690m/z', '0.48_931.2865m/z', '0.65_129.0186m/z', '6.37_201.0562m/z', '2.19_131.0495m/z', '10.00_1169.5321m/z', '8.20_683.4162m/z', '10.63_377.7993n', '0.46_1008.7487n', '7.35_249.1815m/z', '8.59_282.3725m/z', '10.79_151.9800m/z', '0.47_974.8115m/z', '0.48_335.8381n', '8.72_987.7060m/z', '10.64_410.7874m/z', '10.64_265.8685m/z', '8.93_420.2688m/z', '3.16_220.1687m/z', '10.08_338.8438m/z', '6.43_201.0555m/z', '0.47_1118.7695m/z', '0.53_165.9259m/z', '7.76_551.0781m/z', '0.47_804.8427m/z', '0.47_703.3637m/z', '5.08_311.2689m/z', '8.78_522.4304n', '10.63_71.9493m/z', '8.54_397.4147m/z']\n",
            "----------\n",
            "# of labels :  182\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "metabolomics data :\n",
            "# of samples :  100\n",
            "# of features :  482\n",
            "labels: [1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZIeKa2fzSwd",
        "outputId": "fb0f554c-2f0e-4c6f-8031-ea19b7ffb2e0"
      },
      "source": [
        "pip install pyscm-ml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyscm-ml in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyscm-ml) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV-ZH6dx5SpV"
      },
      "source": [
        "result = RandomScmClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOcyLM0H1YVS",
        "outputId": "d9c67129-43d3-4084-f7a3-e66836507ca7"
      },
      "source": [
        "print(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 0.0 ... 1084.074486 91004.26917 -145.43122140000003]\n",
            " [0.0 0.0 0.0 ... 1106.098664 100736.478 -170.776811]\n",
            " [0.0 0.0 0.0 ... 1122.8278990000001 94610.99892 -140.8011661]\n",
            " ...\n",
            " [0.0 0.0 0.0 ... 1024.050104 70873.17068 218.3647556]\n",
            " [0.0 0.0 0.0 ... 1107.663733 72563.38294 234.9694451]\n",
            " [0.0 0.0 0.0 ... 1049.796854 72014.0392 216.1274312]] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI2g6UKD1V78"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmrvlJmm1lAK",
        "outputId": "5fcbaba5-437c-424e-e423-9fe7fe42ad2f"
      },
      "source": [
        "print(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 0.0 ... 1169.085267 71840.17199 223.142849]\n",
            " [0.0 0.0 0.0 ... 1087.592648 76479.99123 534.2568309999999]\n",
            " [0.0 0.0 0.0 ... 1161.018205 73361.0715 301.5213309]\n",
            " ...\n",
            " [0.0 0.0 0.0 ... 1049.796854 72014.0392 216.1274312]\n",
            " [0.0 0.0 0.0 ... 1135.444097 79730.1114 -485.48692530000005]\n",
            " [0.0 0.0 0.0 ... 1071.039711 88879.55426 -170.47225490000002]] [0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO9fAL45PveR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhMzs_dmPyWn"
      },
      "source": [
        "#### Bootstrapping ####\n",
        "########################################################\n",
        "# Creating empty list to hold accuracy values\n",
        "AccuracyValues=[]\n",
        "n_times=60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq6xqiFNP06f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fbc8a3-aab9-4b8a-efc4-336d8c9edf7d"
      },
      "source": [
        "## Performing bootstrapping\n",
        "from sklearn import metrics\n",
        "for i in range(n_times):\n",
        "    #Split the data into training and testing set\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Changing the seed value for each iteration\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42+i)\n",
        "    result = RandomScmClassifier()\n",
        "    result.fit(X_train, y_train)\n",
        "    ########################################################\n",
        "    prediction = result.predict(X_test)\n",
        "    Accuracy=metrics.accuracy_score(y_test, prediction)\n",
        "    print(Accuracy)\n",
        "    AccuracyValues.append((Accuracy))\n",
        "    #print(Accuracy)\n",
        "    print(AccuracyValues)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "[1.0]\n",
            "1.0\n",
            "[1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QJjZEWAP4ef",
        "outputId": "dbd5146d-d18b-4788-a0b5-331f9c105aec"
      },
      "source": [
        "###### Single Decision Tree Regression in Python #######\n",
        "    #choose from different tunable hyper parameters\n",
        "    #RegModel = tree.DecisionTreeRegressor(max_depth=3,criterion='mse')\n",
        " \n",
        "    #Creating the model on Training Data\n",
        "    #DTree=RegModel.fit(X_train,y_train)\n",
        "    #prediction=DTree.predict(X_test)\n",
        " \n",
        "    #Measuring accuracy on Testing Data\n",
        "#Accuracy=100- (np.mean(np.abs((y_test - prediction) / y_test)) * 100)\n",
        "    \n",
        "    # Storing accuracy values\n",
        "#AccuracyValues.append(np.round(Accuracy))\n",
        "    \n",
        "################################################\n",
        "# Result of all bootstrapping trials\n",
        "print(AccuracyValues)\n",
        " \n",
        "# Final accuracy\n",
        "print('Final average accuracy',np.mean(AccuracyValues))\n",
        "#print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_final ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 0.9]\n",
            "Final average accuracy 0.9783333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmUj02i1QBuk",
        "outputId": "cbafce0d-0d72-4edd-ce05-babe4291180f"
      },
      "source": [
        "print(prediction)\n",
        "print(prediction.astype(int).sum())\n",
        "print(len(y_test))\n",
        "#(prediction).astype(int).sum()/len(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 0 0 1 0 1 1 1]\n",
            "7\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loSDBFEw0i__"
      },
      "source": [
        "result.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI3elXul61BW",
        "outputId": "2e009325-36f5-4eb9-a375-22ae424c69d3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "y_final = result.predict(X_test)\n",
        "#result.accuracy_score(y_test, y_final )\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_final ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}