{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DecisionTreeProteomicThibault",
      "provenance": [],
      "authorship_tag": "ABX9TyPm4q6YHE/QHByP4V2Ejopj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/recover/blob/master/DecisionTreeProteomicThibault.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HcYRZzKYV7s",
        "outputId": "fdb301e9-7497-450e-dd95-e78c068e8841"
      },
      "source": [
        "### Pour Thibault, il y a rien ici, c'est la première partie du code qui nettoie les données et les met en view\n",
        "\"\"\"\n",
        "This script extract a data and metadata from the files\n",
        "it gives dataframes and a labels file\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "data_path = ''\n",
        "\n",
        "#print('\\nMETADATA :')\n",
        "metadata_filename = data_path + '/content/metadata.csv'\n",
        "meta_df = pd.read_csv(metadata_filename)\n",
        "#print(meta_df.columns)\n",
        "meta_df.columns = ['#', 'plate', '-', 'symptoms'] + list(meta_df)[4:]\n",
        "#print(meta_df.columns)\n",
        "#print('available metadata :', list(meta_df))\n",
        "meta_idx = meta_df['ID'].to_list()\n",
        "meta_label = meta_df['symptoms'].to_list()\n",
        "#print('------------------')\n",
        "#print(list(zip(meta_idx, meta_label)))\n",
        "#print('------------------')\n",
        "meta_id_label_dict = {str(k): 1 if v=='S' else 0 for k, v in zip(meta_idx, meta_label)}\n",
        "\n",
        "#DF1 : proteomics\n",
        "#print('\\nPROEOMICS DATA :')\n",
        "proteomics_data_filename = '/content/proteomics.csv'\n",
        "\n",
        "dim_df = pd.read_csv(proteomics_data_filename, nrows=1)\n",
        "#print('--------')\n",
        "#print(dim_df)\n",
        "#print('--------')\n",
        "dim = len(list(dim_df))\n",
        "#print(dim)\n",
        "#print('------------')\n",
        "print('# of columns in source csv file :', dim)\n",
        "all_cols = [i for i in range(dim)]\n",
        "\n",
        "print('--------')\n",
        "feat_cols = all_cols[1:-4]\n",
        "print(feat_cols)\n",
        "print('--------')\n",
        "samplesidx_col = [0]\n",
        "\n",
        "feat_df = pd.read_csv(proteomics_data_filename, skiprows=4, nrows=1, dtype=str, usecols=feat_cols)\n",
        "features = list(feat_df)\n",
        "print('# of features : ', len(features))\n",
        "print('first feature :', features[0])\n",
        "print('last feature :', features[-1])\n",
        "\n",
        "idx_df = pd.read_csv(proteomics_data_filename, skiprows=6, index_col=0, skipfooter=4, usecols=[0], engine='python')\n",
        "idx = list(idx_df.index.values)\n",
        "print('# of idx : ', len(idx))\n",
        "print('first id :', idx[0])\n",
        "print('last id :', idx[-1])\n",
        "\n",
        "df1 = pd.read_csv(proteomics_data_filename, skiprows=6, dtype=np.float32, skipfooter=4, usecols=feat_cols, engine='python')\n",
        "assert df1.shape[0] == len(idx)\n",
        "assert df1.shape[1] == len(features)\n",
        "\n",
        "df1['idx'] = idx\n",
        "df1.set_index('idx', inplace=True)\n",
        "df1.columns = features\n",
        "print('# of Nan values :', df1.isna().sum().sum())\n",
        "\n",
        "#clean data of samples that are not in metadata :\n",
        "idx = df1.index.values\n",
        "y = []\n",
        "for k in range(len(idx)):\n",
        "    id = idx[k]\n",
        "    if id in meta_id_label_dict:\n",
        "        y.append(meta_id_label_dict[id])\n",
        "    else:\n",
        "        # we will not put this sample in the dataset\n",
        "        #print('sample to remove because of unknown label:', k, id)\n",
        "        y.append('to_remove')\n",
        "df1['label'] = y\n",
        "df1 = df1[df1.label != 'to_remove']\n",
        "\n",
        "#create X and y matrices for ML :\n",
        "y = list(df1['label'])\n",
        "del df1['label']\n",
        "print('---------')\n",
        "print(df1)\n",
        "print('---------')\n",
        "X = df1.to_numpy()\n",
        "print('proteomics data :')\n",
        "print('# of samples : ', df1.shape[0])\n",
        "print('# of features : ', df1.shape[1])\n",
        "print('labels:', list(dict.fromkeys(y)))\n",
        "\n",
        "\n",
        "## save X and y in pickles if you want :\n",
        "##data_name = 'recover_multiomics_'\n",
        "##feat_dict = {k: str(v) for k, v in zip(range(len(list(df))), list(df))}\n",
        "##with open(data_path + data_name + 'feat_dict', 'wb') as fo:\n",
        "##    pkl.dump(feat_dict, fo)\n",
        "##with open(data_path + data_name + 'X', 'wb') as fo:df\n",
        "##            pkl.dump(X, fo)\n",
        "##with open(data_path + data_name + 'y', 'wb') as fo:\n",
        "##            pkl.dump(y, fo)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of columns in source csv file : 189\n",
            "--------\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]\n",
            "--------\n",
            "# of features :  184\n",
            "first feature : Q96SB3\n",
            "last feature : P09603\n",
            "# of idx :  104\n",
            "first id : 5-139\n",
            "last id : 3-043\n",
            "# of Nan values : 0\n",
            "---------\n",
            "        Q96SB3   P16278   O75475   Q05516  ...  P05113.1   P00813   P01374    P09603\n",
            "idx                                        ...                                      \n",
            "5-139  1.61741  1.31276  3.20077  1.28369  ...   0.12752  5.46542  5.97368  10.40443\n",
            "1-039  2.79530  2.12398  2.90156  1.55239  ...   0.88631  5.24029  5.07646  10.14180\n",
            "1-062  1.69202  2.07015  2.07562  0.85847  ...   0.18121  5.34243  4.89149  10.02279\n",
            "1-040  1.62496  1.67346  2.30191  1.09831  ...   2.11383  5.54121  5.57492  10.13417\n",
            "2-044  1.69246  1.94635  1.87725  0.84233  ...   0.60991  4.98001  4.80933  10.04586\n",
            "...        ...      ...      ...      ...  ...       ...      ...      ...       ...\n",
            "1-073  1.40763  2.32194  2.53494  0.55014  ...   0.31641  5.31388  4.78352  10.50358\n",
            "2-108  1.73766  1.44918  2.23165  0.44999  ...   0.37494  5.44716  5.22955  10.13842\n",
            "5-114  2.94638  1.45486  3.31485  0.93572  ...   2.98474  5.34230  5.30634  10.16184\n",
            "5-075  2.09489  2.70359  2.17787  1.16964  ...   0.76538  5.14975  5.37112  10.40146\n",
            "3-043  2.23329  2.41564  3.45536  0.91918  ...   0.87126  5.34694  5.08457  10.35240\n",
            "\n",
            "[100 rows x 184 columns]\n",
            "---------\n",
            "proteomics data :\n",
            "# of samples :  100\n",
            "# of features :  184\n",
            "labels: [1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX1VuWrzY2o0",
        "outputId": "8b8ad080-f933-4052-a3ef-336af0e5b390"
      },
      "source": [
        "### Pour Thibault, j'utilise la méthode RandomGrid poru trouver mes meilleurs paramètres (ici j'ai trouvé ce set de parametres sur Internet)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from pprint import pprint\n",
        "random_grid = {'max_depth': [None, 2, 5, 10],\n",
        "               'min_samples_leaf': [1, 5, 10],\n",
        "               'min_samples_split': [2, 10, 20],\n",
        "               \"criterion\": [\"gini\", \"entropy\"],\n",
        "               \"max_leaf_nodes\": [None, 5, 10, 20]}\n",
        "pprint(random_grid)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'criterion': ['gini', 'entropy'],\n",
            " 'max_depth': [None, 2, 5, 10],\n",
            " 'max_leaf_nodes': [None, 5, 10, 20],\n",
            " 'min_samples_leaf': [1, 5, 10],\n",
            " 'min_samples_split': [2, 10, 20]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLQfhwVjcEn6",
        "outputId": "0f16ea40-073d-48da-a6cc-ff45a4deb0bd"
      },
      "source": [
        "### Pour Thibault, ici je teste mes différents paramètres pour choisir les meilleurs, puis après les meilleurs sont dans la prochaine cellule \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "tree_clf = DecisionTreeClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "tree_clf = RandomizedSearchCV(estimator = tree_clf, param_distributions = random_grid, n_iter = 200, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "tree_clf.fit(X_train, y_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    5.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, error_score=nan,\n",
              "                   estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                    class_weight=None,\n",
              "                                                    criterion='gini',\n",
              "                                                    max_depth=None,\n",
              "                                                    max_features=None,\n",
              "                                                    max_leaf_nodes=None,\n",
              "                                                    min_impurity_decrease=0.0,\n",
              "                                                    min_impurity_split=None,\n",
              "                                                    min_samples_leaf=1,\n",
              "                                                    min_samples_split=2,\n",
              "                                                    min_weight_fraction_leaf=0.0,\n",
              "                                                    presort='deprecated',\n",
              "                                                    random_state=None,\n",
              "                                                    splitter='best'),\n",
              "                   iid='deprecated', n_iter=200, n_jobs=-1,\n",
              "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
              "                                        'max_depth': [None, 2, 5, 10],\n",
              "                                        'max_leaf_nodes': [None, 5, 10, 20],\n",
              "                                        'min_samples_leaf': [1, 5, 10],\n",
              "                                        'min_samples_split': [2, 10, 20]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e_yM4T1aJvY",
        "outputId": "9c4f0c0c-ffc3-4e9f-844e-001e31273545"
      },
      "source": [
        "### Pour Thibault, ici soit je fais mon train-test standard 80/20, puis j'entraine le modèle et calcule l'accuracy, puis après j'essaie le Nootstarpping a la prochaien cellule.  \n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "tree_clf = DecisionTreeClassifier(criterion='gini', max_depth= 10, max_leaf_nodes= 10, min_samples_leaf = 1, min_samples_split = 10)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "predictions = tree_clf.predict(X_test, check_input=True)\n",
        "#print(len(predictions), predictions)\n",
        "#(predictions).astype(int).sum()/len(y_test)\n",
        "accuracy_score(y_test, predictions)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-yyeGGsanI5",
        "outputId": "6ef418c7-7222-4f90-c935-b613322ea6ff"
      },
      "source": [
        "###  Boostraping , Pour Thibault, tu peux essayer ici, mais a chaque fosi que je roule cette cellule, j'obtiens un résultat différent.\n",
        "# Creating empty list to hold accuracy values\n",
        "AccuracyValues=[]\n",
        "n_times=30\n",
        "\n",
        "## bootstrapping\n",
        "from sklearn import metrics\n",
        "for i in range(n_times):\n",
        "    #Split the data into training and testing set\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Changing the seed value for each iteration\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42+i)\n",
        "    clf = DecisionTreeClassifier(criterion='gini', max_depth= 10, max_leaf_nodes= 10, min_samples_leaf = 1, min_samples_split = 10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    ########################################################\n",
        "    predictions = clf.predict(X_test, check_input=True)\n",
        "    Accuracy=metrics.accuracy_score(y_test, predictions)\n",
        "    AccuracyValues.append(np.round(Accuracy))\n",
        "\n",
        "# Result of all bootstrapping trials\n",
        "print(y_test, predictions)\n",
        "print(AccuracyValues)\n",
        " \n",
        "# Final accuracy\n",
        "print('Final average accuracy',np.mean(AccuracyValues))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0] [1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0]\n",
            "[0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]\n",
            "Final average accuracy 0.4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}