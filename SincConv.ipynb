{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SincConv.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNSbQGsFJmKulZ1ac7J7Gvk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/recover/blob/master/SincConv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZeDMKDX0QBQ"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class SincConv(nn.Module):\n",
        "    \"\"\"This function implements SincConv (SincNet).\n",
        "\n",
        "    M. Ravanelli, Y. Bengio, \"Speaker Recognition from raw waveform with\n",
        "    SincNet\", in Proc. of  SLT 2018 (https://arxiv.org/abs/1808.00158)\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    input_shape : tuple\n",
        "        The shape of the input. Alternatively use ``in_channels``.\n",
        "    in_channels : int\n",
        "        The number of input channels. Alternatively use ``input_shape``.\n",
        "    out_channels : int\n",
        "        It is the number of output channels.\n",
        "    kernel_size: int\n",
        "        Kernel size of the convolutional filters.\n",
        "    stride : int\n",
        "        Stride factor of the convolutional filters. When the stride factor > 1,\n",
        "        a decimation in time is performed.\n",
        "    dilation : int\n",
        "        Dilation factor of the convolutional filters.\n",
        "    padding : str\n",
        "        (same, valid, causal). If \"valid\", no padding is performed.\n",
        "        If \"same\" and stride is 1, output shape is the same as the input shape.\n",
        "        \"causal\" results in causal (dilated) convolutions.\n",
        "    padding_mode : str\n",
        "        This flag specifies the type of padding. See torch.nn documentation\n",
        "        for more information.\n",
        "    groups : int\n",
        "        This option specifies the convolutional groups. See torch.nn\n",
        "        documentation for more information.\n",
        "    bias : bool\n",
        "        If True, the additive bias b is adopted.\n",
        "    sample_rate : int,\n",
        "        Sampling rate of the input signals. It is only used for sinc_conv.\n",
        "    min_low_hz : float\n",
        "        Lowest possible frequency (in Hz) for a filter. It is only used for\n",
        "        sinc_conv.\n",
        "    min_low_hz : float\n",
        "        Lowest possible value (in Hz) for a filter bandwidth.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> inp_tensor = torch.rand([10, 16000])\n",
        "    >>> conv = SincConv(input_shape=inp_tensor.shape, out_channels=25, kernel_size=11)\n",
        "    >>> out_tensor = conv(inp_tensor)\n",
        "    >>> out_tensor.shape\n",
        "    torch.Size([10, 16000, 25])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        input_shape=None,\n",
        "        in_channels=None,\n",
        "        stride=1,\n",
        "        dilation=1,\n",
        "        padding=\"same\",\n",
        "        padding_mode=\"reflect\",\n",
        "        sample_rate=16000,\n",
        "        min_low_hz=50,\n",
        "        min_band_hz=50,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.padding = padding\n",
        "        self.padding_mode = padding_mode\n",
        "        self.sample_rate = sample_rate\n",
        "        self.min_low_hz = min_low_hz\n",
        "        self.min_band_hz = min_band_hz\n",
        "\n",
        "        # input shape inference\n",
        "        if input_shape is None and in_channels is None:\n",
        "            raise ValueError(\"Must provide one of input_shape or in_channels\")\n",
        "\n",
        "        if in_channels is None:\n",
        "            in_channels = self._check_input_shape(input_shape)\n",
        "\n",
        "        # Initialize Sinc filters\n",
        "        self._init_sinc_conv()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the convolution.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, channel)\n",
        "            input to convolve. 2d or 4d tensors are expected.\n",
        "\n",
        "        \"\"\"\n",
        "        x = x.transpose(1, -1)\n",
        "        self.device = x.device\n",
        "\n",
        "        unsqueeze = x.ndim == 2\n",
        "        if unsqueeze:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        if self.padding == \"same\":\n",
        "            x = self._manage_padding(\n",
        "                x, self.kernel_size, self.dilation, self.stride\n",
        "            )\n",
        "\n",
        "        elif self.padding == \"causal\":\n",
        "            num_pad = (self.kernel_size - 1) * self.dilation\n",
        "            x = F.pad(x, (num_pad, 0))\n",
        "\n",
        "        elif self.padding == \"valid\":\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Padding must be 'same', 'valid' or 'causal'. Got %s.\"\n",
        "                % (self.padding)\n",
        "            )\n",
        "\n",
        "        sinc_filters = self._get_sinc_filters()\n",
        "\n",
        "        wx = F.conv1d(\n",
        "            x,\n",
        "            sinc_filters,\n",
        "            stride=self.stride,\n",
        "            padding=0,\n",
        "            dilation=self.dilation,\n",
        "        )\n",
        "\n",
        "        if unsqueeze:\n",
        "            wx = wx.squeeze(1)\n",
        "\n",
        "        wx = wx.transpose(1, -1)\n",
        "\n",
        "        return wx\n",
        "\n",
        "\n",
        "    def _check_input_shape(self, shape):\n",
        "        \"\"\"Checks the input shape and returns the number of input channels.\n",
        "        \"\"\"\n",
        "\n",
        "        if len(shape) == 2:\n",
        "            in_channels = 1\n",
        "        elif len(shape) == 3:\n",
        "            in_channels = 1\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"sincconv expects 2d or 3d inputs. Got \" + str(len(shape))\n",
        "            )\n",
        "\n",
        "        # Kernel size must be odd\n",
        "        if self.kernel_size % 2 == 0:\n",
        "            raise ValueError(\n",
        "                \"The field kernel size must be an odd number. Got %s.\"\n",
        "                % (self.kernel_size)\n",
        "            )\n",
        "        return in_channels\n",
        "\n",
        "    def _get_sinc_filters(self,):\n",
        "        \"\"\"This functions creates the sinc-filters to used for sinc-conv.\n",
        "        \"\"\"\n",
        "        # Computing the low frequencies of the filters\n",
        "        low = self.min_low_hz + torch.abs(self.low_hz_)\n",
        "\n",
        "        # Setting minimum band and minimum freq\n",
        "        high = torch.clamp(\n",
        "            low + self.min_band_hz + torch.abs(self.band_hz_),\n",
        "            self.min_low_hz,\n",
        "            self.sample_rate / 2,\n",
        "        )\n",
        "        band = (high - low)[:, 0]\n",
        "\n",
        "        # Passing from n_ to the corresponding f_times_t domain\n",
        "        self.n_ = self.n_.to(self.device)\n",
        "        self.window_ = self.window_.to(self.device)\n",
        "        f_times_t_low = torch.matmul(low, self.n_)\n",
        "        f_times_t_high = torch.matmul(high, self.n_)\n",
        "\n",
        "        # Left part of the filters.\n",
        "        band_pass_left = ((torch.sin(f_times_t_high) - torch.sin(f_times_t_low))/ (self.n_ / 2)) * self.window_\n",
        "\n",
        "        # Central element of the filter\n",
        "        band_pass_center = 2 * band.view(-1, 1)\n",
        "\n",
        "        # Right part of the filter (sinc filters are symmetric)\n",
        "        band_pass_right = torch.flip(band_pass_left, dims=[1])\n",
        "\n",
        "        # Combining left, central, and right part of the filter\n",
        "        band_pass = torch.cat([band_pass_left, band_pass_center, band_pass_right], dim=1)\n",
        "\n",
        "        # Amplitude normalization\n",
        "        band_pass = band_pass / (2 * band[:, None])\n",
        "\n",
        "        # Setting up the filter coefficients\n",
        "        filters = band_pass.view(self.out_channels, 1, self.kernel_size)\n",
        "\n",
        "        return filters\n",
        "\n",
        "    def _init_sinc_conv(self):\n",
        "        \"\"\"Initializes the parameters of the sinc_conv layer.\"\"\"\n",
        "\n",
        "        # Initialize filterbanks such that they are equally spaced in Mel scale\n",
        "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
        "\n",
        "        mel = torch.linspace(\n",
        "            self._to_mel(self.min_low_hz),\n",
        "            self._to_mel(high_hz),\n",
        "            self.out_channels + 1,\n",
        "        )\n",
        "\n",
        "        hz = self._to_hz(mel)\n",
        "\n",
        "        # Filter lower frequency and bands\n",
        "        self.low_hz_ = hz[:-1].unsqueeze(1)\n",
        "        self.band_hz_ = (hz[1:] - hz[:-1]).unsqueeze(1)\n",
        "\n",
        "        # Maiking freq and bands learnable\n",
        "        self.low_hz_ = nn.Parameter(self.low_hz_)\n",
        "        self.band_hz_ = nn.Parameter(self.band_hz_)\n",
        "\n",
        "        # Hamming window\n",
        "        n_lin = torch.linspace(0, (self.kernel_size / 2) - 1, steps=int((self.kernel_size / 2)))\n",
        "        self.window_ = 0.54 - 0.46 * torch.cos(2 * math.pi * n_lin / self.kernel_size)\n",
        "\n",
        "        # Time axis  (only half is needed due to symmetry)\n",
        "        n = (self.kernel_size - 1) / 2.0\n",
        "        self.n_ = (2 * math.pi * torch.arange(-n, 0).view(1, -1) / self.sample_rate)\n",
        "\n",
        "    def _to_mel(self, hz):\n",
        "        \"\"\"Converts frequency in Hz to the mel scale.\n",
        "        \"\"\"\n",
        "        return 2595 * np.log10(1 + hz / 700)\n",
        "\n",
        "    def _to_hz(self, mel):\n",
        "        \"\"\"Converts frequency in the mel scale to Hz.\n",
        "        \"\"\"\n",
        "        return 700 * (10 ** (mel / 2595) - 1)\n",
        "\n",
        "    def _manage_padding(self, x, kernel_size: int, dilation: int, stride: int,):\n",
        "        \"\"\"This function performs zero-padding on the time axis\n",
        "        such that their lengths is unchanged after the convolution.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor\n",
        "            Input tensor.\n",
        "        kernel_size : int\n",
        "            Size of kernel.\n",
        "        dilation : int\n",
        "            Dilation used.\n",
        "        stride : int\n",
        "            Stride.\n",
        "        \"\"\"\n",
        "\n",
        "        # Detecting input shape\n",
        "        L_in = x.shape[-1]\n",
        "\n",
        "        # Time padding\n",
        "        padding = get_padding_elem(L_in, stride, kernel_size, dilation)\n",
        "\n",
        "        # Applying padding\n",
        "        x = F.pad(x, padding, mode=self.padding_mode)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdmxD3Ti_u_y",
        "outputId": "beb8a124-5199-4528-ce00-074ee134fb1c"
      },
      "source": [
        "!pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple BeechSprain==0.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'BeechSprain' candidate (version 0.5 at https://test-files.pythonhosted.org/packages/af/4d/cf86ae59c0c17708676cd7289a0587a760eaa7c820234f23e0a9240479b8/BeechSprain-0.5-py3-none-any.whl#sha256=4699f0a48695bb9767a66bd7eb1abd3c52e2634b5f01685c25c78cb91547fc31 (from https://test.pypi.org/simple/beechsprain/) (requires-python:>=3.6))\n",
            "Reason for being yanked: Out of date\u001b[0m\n",
            "Collecting BeechSprain==0.5\n",
            "\u001b[?25l  Downloading https://test-files.pythonhosted.org/packages/af/4d/cf86ae59c0c17708676cd7289a0587a760eaa7c820234f23e0a9240479b8/BeechSprain-0.5-py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from BeechSprain==0.5) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BeechSprain==0.5) (1.19.5)\n",
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 21.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from BeechSprain==0.5) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from BeechSprain==0.5) (1.4.1)\n",
            "Collecting hyperpyyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/17/e2/63e6353151cb4359f66e93f52152d7d60c1b32c87f5b2e2e58419d2a3711/HyperPyYAML-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from BeechSprain==0.5) (1.0.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://test-files.pythonhosted.org/packages/7a/e5/9100ae18ad5da7f162d93196c13bcfc389c5ac33dcd82a697ed585152dab/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->BeechSprain==0.5) (3.7.4.3)\n",
            "Collecting ruamel.yaml>=0.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/4e/c3105bbbbc662f6a671a505f00ec771e93b5254f09fbb06002af9087071a/ruamel.yaml-0.17.4-py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.7MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 44.2MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.10\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/6e/f652c56bbb2c3d3fca252ffc7c0358597f57a1bbdf484dac683054950c63/ruamel.yaml.clib-0.2.2-cp37-cp37m-manylinux1_x86_64.whl (547kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 37.8MB/s \n",
            "\u001b[?25hInstalling collected packages: torchaudio, ruamel.yaml.clib, ruamel.yaml, pyyaml, hyperpyyaml, sentencepiece, BeechSprain\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed BeechSprain-0.5 hyperpyyaml-1.0.0 pyyaml-5.4.1 ruamel.yaml-0.17.4 ruamel.yaml.clib-0.2.2 sentencepiece-0.1.95 torchaudio-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afrawwv19v8U"
      },
      "source": [
        "import speechbrain as sb\n",
        "import torch.nn as nn\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "from IPython.display import Audio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\"\"\"\n",
        "Downloads and creates data manifest files for Mini LibriSpeech (spk-id).\n",
        "For speaker-id, different senteces of the same speaker must appear in train,\n",
        "validation, and test sets. In this case, these sets are thus derived from\n",
        "splitting the orginal training set intothree chunks.\n",
        "Authors:\n",
        " * Mirco Ravanelli, 2021\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import logging\n",
        "from speechbrain.utils.data_utils import get_all_files, download_file\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "import ssl\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "COVID_URL =\"http://dl.dropboxusercontent.com/s/7kkfr24kwnw3jl1/covidData.tar.gz?dl=0\"\n",
        "SAMPLERATE = 16000\n",
        "\n",
        "\n",
        "def prepare_covid_dataset(\n",
        "    data_folder,\n",
        "    save_json_train,\n",
        "    save_json_valid,\n",
        "    save_json_test,\n",
        "    split_ratio=[80, 10, 10],\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares the json files for the Mini Librispeech dataset.\n",
        "    Downloads the dataset if it is not found in the `data_folder`.\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_folder : str\n",
        "        Path to the folder where the Mini Librispeech dataset is stored.\n",
        "    save_json_train : str\n",
        "        Path where the train data specification file will be saved.\n",
        "    save_json_valid : str\n",
        "        Path where the validation data specification file will be saved.\n",
        "    save_json_test : str\n",
        "        Path where the test data specification file will be saved.\n",
        "    split_ratio: list\n",
        "        List composed of three integers that sets split ratios for train, valid,\n",
        "        and test sets, respecively. For instance split_ratio=[80, 10, 10] will\n",
        "        assign 80% of the sentences to training, 10% for validation, and 10%\n",
        "        for test.\n",
        "    Example\n",
        "    -------\n",
        "    >>> data_folder = '/path/to/mini_librispeech'\n",
        "    >>> prepare_mini_librispeech(data_folder, 'train.json', 'valid.json', 'test.json')\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if this phase is already done (if so, skip it)\n",
        "    if skip(save_json_train, save_json_valid, save_json_test):\n",
        "        logger.info(\"Preparation completed in previous run, skipping.\")\n",
        "        return\n",
        "\n",
        "    # If the dataset doesn't exist yet, download it\n",
        "    train_folder = os.path.join(data_folder, \"covid_data\")\n",
        "    if not check_folders(train_folder):\n",
        "        download_dataset(data_folder)\n",
        "\n",
        "    # List files and create manifest from list\n",
        "    logger.info(\n",
        "        f\"Creating {save_json_train}, {save_json_valid}, and {save_json_test}\"\n",
        "    )\n",
        "    extension = [\".wav\"]\n",
        "    train_folder_negative = os.path.join(data_folder, \"covid_data\", \"covid_negative\")\n",
        "    train_folder_positive = os.path.join(data_folder, \"covid_data\", \"covid_positive\")\n",
        "    \n",
        "    wav_list_negative = get_all_files(train_folder_negative, match_and=extension)\n",
        "    wav_list_positive = get_all_files(train_folder_positive, match_and=extension)\n",
        "    \n",
        "    \n",
        "    # Random split the signal list into train, valid, and test sets.\n",
        "    data_split_neg = split_sets(wav_list_negative, split_ratio)\n",
        "    data_split_pos = split_sets(wav_list_positive, split_ratio)\n",
        "\n",
        "    # Creating json files\n",
        "    create_json(data_split_neg[\"train\"], data_split_pos[\"train\"], save_json_train)\n",
        "    create_json(data_split_neg[\"valid\"], data_split_pos[\"valid\"], save_json_valid)\n",
        "    create_json(data_split_neg[\"test\"], data_split_pos[\"test\"], save_json_test)\n",
        "\n",
        "\n",
        "def create_json(wav_list_neg, wav_list_pos, json_file):\n",
        "    \"\"\"\n",
        "    Creates the json file given a list of wav files.\n",
        "    Arguments\n",
        "    ---------\n",
        "    wav_list : list of str\n",
        "        The list of wav files.\n",
        "    json_file : str\n",
        "        The path of the output json file\n",
        "    \"\"\"\n",
        "    # Processing all the wav files in the list\n",
        "    json_dict = {}\n",
        "    #wav_list_neg = np.array(wav_list_neg)\n",
        "    \n",
        "    for i in range(len(wav_list_pos)):\n",
        "        wav_file_neg = wav_list_neg[i]\n",
        "\n",
        "        wav_file_neg = wav_file_neg.replace(\"._\", \"\")\n",
        "\n",
        "        # Reading the signal (to retrieve duration in seconds)\n",
        "        signal = read_audio(wav_file_neg)\n",
        "        duration = signal.shape[0] / SAMPLERATE\n",
        "\n",
        "        # Manipulate path to get relative path and uttid\n",
        "        path_parts = wav_file_neg.split(os.path.sep)\n",
        "        uttid, _ = os.path.splitext(path_parts[-1])\n",
        "        relative_path = os.path.join(\"{data_root}\", *path_parts[-5:])\n",
        "\n",
        "        # Getting speaker-id from utterance-id\n",
        "        status = 0\n",
        "\n",
        "        # Create entry for this utterance\n",
        "        json_dict[uttid] = {\n",
        "            \"wav\": relative_path,\n",
        "            \"length\": duration,\n",
        "            \"status\": status,\n",
        "        }\n",
        "        \n",
        "        wav_file_pos = wav_list_pos[i]\n",
        "        wav_file_pos = wav_file_pos.replace(\"._\", \"\")\n",
        "\n",
        "        # Reading the signal (to retrieve duration in seconds)\n",
        "        signal = read_audio(wav_file_pos)\n",
        "        duration = signal.shape[0] / SAMPLERATE\n",
        "\n",
        "        # Manipulate path to get relative path and uttid\n",
        "        path_parts = wav_file_pos.split(os.path.sep)\n",
        "        uttid, _ = os.path.splitext(path_parts[-1])\n",
        "        relative_path = os.path.join(\"{data_root}\", *path_parts[-5:])\n",
        "\n",
        "        # Getting speaker-id from utterance-id\n",
        "        status = 1\n",
        "\n",
        "        # Create entry for this utterance\n",
        "        json_dict[uttid] = {\n",
        "            \"wav\": relative_path,\n",
        "            \"length\": duration,\n",
        "            \"status\": 1,\n",
        "        }\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    # Writing the dictionary to the json file\n",
        "    with open(json_file, mode=\"w\") as json_f:\n",
        "        json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "    logger.info(f\"{json_file} successfully created!\")\n",
        "\n",
        "\n",
        "def skip(*filenames):\n",
        "    \"\"\"\n",
        "    Detects if the data preparation has been already done.\n",
        "    If the preparation has been done, we can skip it.\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        if True, the preparation phase can be skipped.\n",
        "        if False, it must be done.\n",
        "    \"\"\"\n",
        "    for filename in filenames:\n",
        "        if not os.path.isfile(filename):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_folders(*folders):\n",
        "    \"\"\"Returns False if any passed folder does not exist.\"\"\"\n",
        "    for folder in folders:\n",
        "        if not os.path.exists(folder):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def split_sets(wav_list, split_ratio):\n",
        "    \"\"\"Randomly splits the wav list into training, validation, and test lists.\n",
        "    Note that a better approach is to make sure that all the classes have the\n",
        "    same proportion of samples (e.g, spk01 should have 80% of samples in\n",
        "    training, 10% validation, 10% test, the same for speaker2 etc.). This\n",
        "    is the approach followed in some recipes such as the Voxceleb one. For\n",
        "    simplicity, we here simply split the full list without necessarly respecting\n",
        "    the split ratio within each class.\n",
        "    Arguments\n",
        "    ---------\n",
        "    wav_lst : list\n",
        "        list of all the signals in the dataset\n",
        "    split_ratio: list\n",
        "        List composed of three integers that sets split ratios for train, valid,\n",
        "        and test sets, respectively. For instance split_ratio=[80, 10, 10] will\n",
        "        assign 80% of the sentences to training, 10% for validation, and 10%\n",
        "        for test.\n",
        "    Returns\n",
        "    ------\n",
        "    dictionary containing train, valid, and test splits.\n",
        "    \"\"\"\n",
        "    # Random shuffle of the list\n",
        "    random.shuffle(wav_list)\n",
        "    tot_split = sum(split_ratio)\n",
        "    tot_snts = len(wav_list)\n",
        "    data_split = {}\n",
        "    splits = [\"train\", \"valid\"]\n",
        "\n",
        "    for i, split in enumerate(splits):\n",
        "        n_snts = int(tot_snts * split_ratio[i] / tot_split)\n",
        "        data_split[split] = wav_list[0:n_snts]\n",
        "        del wav_list[0:n_snts]\n",
        "    data_split[\"test\"] = wav_list\n",
        "\n",
        "    return data_split\n",
        "\n",
        "\n",
        "def download_dataset(destination):\n",
        "    \"\"\"Download dataset and unpack it.\n",
        "    Arguments\n",
        "    ---------\n",
        "    destination : str\n",
        "        Place to put dataset.\n",
        "    \"\"\"\n",
        "    # paste this at the start of code\n",
        "\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "    \n",
        "    train_archive = os.path.join(destination, \"covidData.tar.gz\")\n",
        "    download_file(COVID_URL, train_archive)\n",
        "    shutil.unpack_archive(train_archive, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "Nktbka9f2gAP",
        "outputId": "711a4acc-e6f2-4182-d3a9-782b34b409ed"
      },
      "source": [
        "inp_tensor = torch.rand([10, 16000])\n",
        "inp_tensor.shape\n",
        "conv = SincConv(input_shape=inp_tensor.shape, out_channels=25, kernel_size=11)\n",
        "out_tensor = conv(inp_tensor)\n",
        "# out_tensor.shape\n",
        "    #torch.Size([10, 16000, 25])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-ca5424bd3578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minp_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSincConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minp_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mout_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# out_tensor.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#torch.Size([10, 16000, 25])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-f332336e3e2c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"same\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             x = self._manage_padding(\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-f332336e3e2c>\u001b[0m in \u001b[0;36m_manage_padding\u001b[0;34m(self, x, kernel_size, dilation, stride)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# Time padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_padding_elem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Applying padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_padding_elem' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "1U_cEHWZ8ijn",
        "outputId": "4cb528ac-aa21-477c-9cdd-bb6d8e4e89c2"
      },
      "source": [
        "\"\"\"Downloads and creates data manifest files for Mini LibriSpeech (spk-id).\n",
        "For speaker-id, different senteces of the same speaker must appear in train,\n",
        "validation, and test sets. In this case, these sets are thus derived from\n",
        "splitting the orginal training set intothree chunks.\n",
        "Authors:\n",
        " * Mirco Ravanelli, 2021\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import logging\n",
        "from speechbrain.utils.data_utils import get_all_files, download_file\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "import ssl\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "COVID_URL =\"http://dl.dropboxusercontent.com/s/7kkfr24kwnw3jl1/covidData.tar.gz?dl=0\"\n",
        "SAMPLERATE = 16000\n",
        "\n",
        "\n",
        "def prepare_covid_dataset(\n",
        "    data_folder,\n",
        "    save_json_train,\n",
        "    save_json_valid,\n",
        "    save_json_test,\n",
        "    split_ratio=[80, 10, 10],\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares the json files for the Mini Librispeech dataset.\n",
        "    Downloads the dataset if it is not found in the `data_folder`.\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_folder : str\n",
        "        Path to the folder where the Mini Librispeech dataset is stored.\n",
        "    save_json_train : str\n",
        "        Path where the train data specification file will be saved.\n",
        "    save_json_valid : str\n",
        "        Path where the validation data specification file will be saved.\n",
        "    save_json_test : str\n",
        "        Path where the test data specification file will be saved.\n",
        "    split_ratio: list\n",
        "        List composed of three integers that sets split ratios for train, valid,\n",
        "        and test sets, respecively. For instance split_ratio=[80, 10, 10] will\n",
        "        assign 80% of the sentences to training, 10% for validation, and 10%\n",
        "        for test.\n",
        "    Example\n",
        "    -------\n",
        "    >>> data_folder = '/path/to/mini_librispeech'\n",
        "    >>> prepare_mini_librispeech(data_folder, 'train.json', 'valid.json', 'test.json')\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if this phase is already done (if so, skip it)\n",
        "    if skip(save_json_train, save_json_valid, save_json_test):\n",
        "        logger.info(\"Preparation completed in previous run, skipping.\")\n",
        "        return\n",
        "         # If the dataset doesn't exist yet, download it\n",
        "    train_folder = os.path.join(data_folder, \"covid_data\")\n",
        "    if not check_folders(train_folder):\n",
        "        download_dataset(data_folder)\n",
        "\n",
        "    # List files and create manifest from list\n",
        "    logger.info(\n",
        "        f\"Creating {save_json_train}, {save_json_valid}, and {save_json_test}\"\n",
        "    )\n",
        "    extension = [\".wav\"]\n",
        "    train_folder_negative = os.path.join(data_folder, \"covid_data\", \"covid_negative\")\n",
        "    train_folder_positive = os.path.join(data_folder, \"covid_data\", \"covid_positive\")\n",
        "\n",
        "    wav_list_negative = get_all_files(train_folder_negative, match_and=extension)\n",
        "    wav_list_positive = get_all_files(train_folder_positive, match_and=extension)\n",
        "\n",
        "\n",
        "    # Random split the signal list into train, valid, and test sets.\n",
        "    data_split_neg = split_sets(wav_list_negative, split_ratio)\n",
        "    data_split_pos = split_sets(wav_list_positive, split_ratio)\n",
        "\n",
        "    # Creating json files\n",
        "    create_json(data_split_neg[\"train\"], data_split_pos[\"train\"], save_json_train)\n",
        "    create_json(data_split_neg[\"valid\"], data_split_pos[\"valid\"], save_json_valid)\n",
        "    create_json(data_split_neg[\"test\"], data_split_pos[\"test\"], save_json_test)\n",
        "\n",
        "\n",
        "def create_json(wav_list_neg, wav_list_pos, json_file):\n",
        "    \"\"\"\n",
        "    Creates the json file given a list of wav files.\n",
        "    Arguments\n",
        "    ---------\n",
        "    wav_list : list of str\n",
        "        The list of wav files.\n",
        "    json_file : str\n",
        "        The path of the output json file\n",
        "    \"\"\"\n",
        "    # Processing all the wav files in the list\n",
        "    json_dict = {}\n",
        "    #wav_list_neg = np.array(wav_list_neg)\n",
        "\n",
        "    for i in range(len(wav_list_pos)):\n",
        "        wav_file_neg = wav_list_neg[i]\n",
        "\n",
        "        wav_file_neg = wav_file_neg.replace(\"._\", \"\")\n",
        "\n",
        "        # Reading the signal (to retrieve duration in seconds)\n",
        "        signal = read_audio(wav_file_neg)\n",
        "        duration = signal.shape[0] / SAMPLERATE\n",
        "\n",
        "        # Manipulate path to get relative path and uttid\n",
        "        path_parts = wav_file_neg.split(os.path.sep)\n",
        "        uttid, _ = os.path.splitext(path_parts[-1])\n",
        "        relative_path = os.path.join(\"{data_root}\", *path_parts[-5:])\n",
        "\n",
        "        # Getting speaker-id from utterance-id\n",
        "        status = 0\n",
        "         # Create entry for this utterance\n",
        "        json_dict[uttid] = {\n",
        "            \"wav\": relative_path,\n",
        "            \"length\": duration,\n",
        "            \"status\": status,\n",
        "        }\n",
        "\n",
        "        wav_file_pos = wav_list_pos[i]\n",
        "        wav_file_pos = wav_file_pos.replace(\"._\", \"\")\n",
        "\n",
        "        # Reading the signal (to retrieve duration in seconds)\n",
        "        signal = read_audio(wav_file_pos)\n",
        "        duration = signal.shape[0] / SAMPLERATE\n",
        "\n",
        "        # Manipulate path to get relative path and uttid\n",
        "        path_parts = wav_file_pos.split(os.path.sep)\n",
        "        uttid, _ = os.path.splitext(path_parts[-1])\n",
        "        relative_path = os.path.join(\"{data_root}\", *path_parts[-5:])\n",
        "\n",
        "        # Getting speaker-id from utterance-id\n",
        "        status = 1\n",
        "\n",
        "        # Create entry for this utterance\n",
        "        json_dict[uttid] = {\n",
        "            \"wav\": relative_path,\n",
        "            \"length\": duration,\n",
        "            \"status\": 1,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Writing the dictionary to the json file\n",
        "    with open(json_file, mode=\"w\") as json_f:\n",
        "        json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "    logger.info(f\"{json_file} successfully created!\")\n",
        "\n",
        "\n",
        "def skip(*filenames):\n",
        "    \"\"\"\n",
        "    Detects if the data preparation has been already done.\n",
        "    If the preparation has been done, we can skip it.\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        if True, the preparation phase can be skipped.\n",
        "        if False, it must be done.\n",
        "    \"\"\"\n",
        "    for filename in filenames:\n",
        "        if not os.path.isfile(filename):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_folders(*folders):\n",
        "     \"\"\"Returns False if any passed folder does not exist.\"\"\"\n",
        "    for folder in folders:\n",
        "        if not os.path.exists(folder):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def split_sets(wav_list, split_ratio):\n",
        "    \"\"\"Randomly splits the wav list into training, validation, and test lists.\n",
        "    Note that a better approach is to make sure that all the classes have the\n",
        "    same proportion of samples (e.g, spk01 should have 80% of samples in\n",
        "    training, 10% validation, 10% test, the same for speaker2 etc.). This\n",
        "    is the approach followed in some recipes such as the Voxceleb one. For\n",
        "    simplicity, we here simply split the full list without necessarly respecting\n",
        "    the split ratio within each class.\n",
        "    Arguments\n",
        "    ---------\n",
        "    wav_lst : list\n",
        "        list of all the signals in the dataset\n",
        "    split_ratio: list\n",
        "        List composed of three integers that sets split ratios for train, valid,\n",
        "        and test sets, respectively. For instance split_ratio=[80, 10, 10] will\n",
        "        assign 80% of the sentences to training, 10% for validation, and 10%\n",
        "        for test.\n",
        "    Returns\n",
        "    ------\n",
        "    dictionary containing train, valid, and test splits.\n",
        "    \"\"\"\n",
        "    # Random shuffle of the list\n",
        "    random.shuffle(wav_list)\n",
        "    tot_split = sum(split_ratio)\n",
        "    tot_snts = len(wav_list)\n",
        "    data_split = {}\n",
        "    splits = [\"train\", \"valid\"]\n",
        "\n",
        "    for i, split in enumerate(splits):\n",
        "        n_snts = int(tot_snts * split_ratio[i] / tot_split)\n",
        "        data_split[split] = wav_list[0:n_snts]\n",
        "        del wav_list[0:n_snts]\n",
        "    data_split[\"test\"] = wav_list\n",
        "\n",
        "    return data_split\n",
        "\n",
        "\n",
        "def download_dataset(destination):\n",
        "    \"\"\"Download dataset and unpack it.\n",
        "    Arguments\n",
        "    ---------\n",
        "    destination : str\n",
        "        Place to put dataset.\n",
        "    \"\"\"\n",
        "    # paste this at the start of code\n",
        "\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "    train_archive = os.path.join(destination, \"covidData.tar.gz\")\n",
        "    download_file(COVID_URL, train_archive)\n",
        "    shutil.unpack_archive(train_archive, destination)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-b733ab8acacd>\"\u001b[0;36m, line \u001b[0;32m173\u001b[0m\n\u001b[0;31m    for folder in folders:\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK2c6CrREbGH"
      },
      "source": [
        "###layerNorm\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"Applies layer normalization to the input tensor.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    input_shape : tuple\n",
        "        The expected shape of the input.\n",
        "    eps : float\n",
        "        This value is added to std deviation estimation to improve the numerical\n",
        "        stability.\n",
        "    elementwise_affine : bool\n",
        "        If True, this module has learnable per-element affine parameters\n",
        "        initialized to ones (for weights) and zeros (for biases).\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> input = torch.randn(100, 101, 128)\n",
        "    >>> norm = LayerNorm(input_shape=input.shape)\n",
        "    >>> output = norm(input)\n",
        "    >>> output.shape\n",
        "    torch.Size([100, 101, 128])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=None,\n",
        "        input_shape=None,\n",
        "        eps=1e-05,\n",
        "        elementwise_affine=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "\n",
        "        if input_shape is not None:\n",
        "            input_size = input_shape[2:]\n",
        "\n",
        "        self.norm = torch.nn.LayerNorm(\n",
        "            input_size,\n",
        "            eps=self.eps,\n",
        "            elementwise_affine=self.elementwise_affine,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the normalized input tensor.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, channels)\n",
        "            input to normalize. 3d or 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCSlqKaBt12P",
        "outputId": "ed9cd1ee-69a1-4d6e-957d-03a8e11e9cca"
      },
      "source": [
        "import torch\n",
        "input = torch.randn(100, 101, 128)\n",
        "input.size()\n",
        "import numpy as np\n",
        "\n",
        "a = np .array([[1,5,3],[2,5]])\n",
        "a.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEaFcMoX_Y27"
      },
      "source": [
        "###LeakyRelu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QkwWPo0_c_4"
      },
      "source": [
        "#CNN/Layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENAjDADR7wWl"
      },
      "source": [
        "###Softmax\n",
        "import torch\n",
        "import logging\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "[docs]class Softmax(torch.nn.Module):\n",
        "    \"\"\"Computes the softmax of a 2d, 3d, or 4d input tensor.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    apply_log : bool\n",
        "        Whether to apply the log function before softmax.\n",
        "    dim : int\n",
        "        If the dimension where softmax is applied.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> classifier = Softmax()\n",
        "    >>> inputs = torch.rand(10, 50, 40)\n",
        "    >>> output = classifier(inputs)\n",
        "    >>> output.shape\n",
        "    torch.Size([10, 50, 40])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, apply_log=False, dim=-1):\n",
        "        super().__init__()\n",
        "\n",
        "        if apply_log:\n",
        "            self.act = torch.nn.LogSoftmax(dim=dim)\n",
        "        else:\n",
        "            self.act = torch.nn.Softmax(dim=dim)\n",
        "\n",
        "[docs]    def forward(self, x):\n",
        "        \"\"\"Returns the softmax of the input tensor.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor\n",
        "            Input tensor.\n",
        "        \"\"\"\n",
        "        # Reshaping the tensors\n",
        "        dims = x.shape\n",
        "\n",
        "        if len(dims) == 3:\n",
        "            x = x.reshape(dims[0] * dims[1], dims[2])\n",
        "\n",
        "        if len(dims) == 4:\n",
        "            x = x.reshape(dims[0] * dims[1], dims[2], dims[3])\n",
        "\n",
        "        x_act = self.act(x)\n",
        "\n",
        "        # Retrieving the original shape format\n",
        "        if len(dims) == 3:\n",
        "            x_act = x_act.reshape(dims[0], dims[1], dims[2])\n",
        "\n",
        "        if len(dims) == 4:\n",
        "            x_act = x_act.reshape(dims[0], dims[1], dims[2], dims[3])\n",
        "\n",
        "        return x_act\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}